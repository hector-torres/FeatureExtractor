{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Note: requires massive refactor for updated application design.\n",
    "\n",
    "# The Tweet Summarizer\n",
    "\n",
    "This application takes in raw tweet objects from a database, then extracts summaries and \n",
    "entities from them using rule-based and machine learning-based approaches. \n",
    "\n",
    "*This is a demonstration attempt at detecting \"events\" from a hand-picked selection of \n",
    "Twitter accounts. Therefore, this demo invoked limited use of advanced spam- and noise-detection \n",
    "techniques, since it is assumed that this dataset will be free from such things. \n",
    "\n",
    "### The Process\n",
    "\n",
    "In order to filter raw tweet objects for noise, then summarize them and extract their \n",
    "natural entities, we will use the following approach.\n",
    "\n",
    "1. Reading in the Dataset\n",
    "2. Pre-processing the Data\n",
    "3. Vectorizing the Data\n",
    "4. Feature Selection\n",
    "5. Building the Machine Learning Classifiers\n",
    "\n",
    "Let's begin!\n",
    "\n",
    "## 1. Reading in the Dataset\n",
    "Before we begin the feature extraction process, we need to connect to the database \n",
    "and pull down information. Two things to note as we begin this process: \n",
    "* database connection information is held in a separate configuration file inside \"/Users/\n",
    "$User/Quantum/Event Detector/Twitter Event Detector/\"\n",
    "* since the idea is to gather real-time data, the idea would be to poll the database every \n",
    "15-45 seconds. However, since we are merely testing the concept here, initial polls will be \n",
    "conducted every 120-300 seconds. \n",
    "\n",
    "Here, we connect to the database and read the last 100 entries (changeable in the first variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hectortorres/Documents/Codebases/Python Sandbox/JupyterTweetSummarizer/venv/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "number_of_rows = 100\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "def config_file_reader(API_caller: str) -> tuple:\n",
    "    \"\"\"\n",
    "    A common configuration file reader.\n",
    "    \n",
    "    Reads data from a common configuration file, determining which fields to call depending \n",
    "    on the API caller passed to it.\n",
    "    \n",
    "    :param API_caller:(str) the name of the service calling this API\n",
    "     \n",
    "    :return: (tuple) a tuple of strings of each configuration returned for the called service\n",
    "    \"\"\"\n",
    "    # \"/Users/$User/Quantum/Event Detector/Twitter Event Detector/\". \n",
    "    home_directory_path = os.path.expanduser(\"~\")\n",
    "    logger_directory_path = os.path.join(home_directory_path, \"Quantum\", \"Event Detector\", \n",
    "                                         \"Twitter Event Detector\", \"Logs\")\n",
    "    config_directory_path = os.path.join(home_directory_path, \"Quantum\", \"Event Detector\", \n",
    "                                         \"Twitter Event Detector\", \"Common\")\n",
    "    config_file_path = os.path.join(config_directory_path, \"config.ini\")\n",
    "\n",
    "    # instantiates the configuration parser\n",
    "    config = configparser.ConfigParser()\n",
    "    \n",
    "    # if config files exists, proceed: else, create directory structure, then fail gracefully\n",
    "    if os.path.exists(config_file_path):\n",
    "        config.read(config_file_path)\n",
    "    else:\n",
    "        os.makedirs(config_directory_path)\n",
    "        print(\"No config file found in \" + config_directory_path + \n",
    "              \". Please place a configuration file into this directory and try again.\")\n",
    "        \n",
    "    if API_caller == \"data_access_object\":\n",
    "        database_type = config[\"DATABASE\"][\"type\"]\n",
    "        database_host = config[\"DATABASE\"][\"host\"]\n",
    "        database_name = config[\"DATABASE\"][\"database_name\"]\n",
    "        database_user = config[\"DATABASE\"][\"user\"]\n",
    "        database_password = config[\"DATABASE\"][\"password\"]\n",
    "        database_instance_id = config[\"DATABASE\"][\"database_instance_id\"]\n",
    "        database_port = config[\"DATABASE\"][\"database_port\"]\n",
    "        return database_type, database_host, database_name, database_user, database_password, \\\n",
    "               database_instance_id, database_port\n",
    "    elif API_caller == \"logger_setup\":\n",
    "        return logger_directory_path,\n",
    "    elif API_caller == \"languages\":\n",
    "        languages = config[\"LANGUAGES\"][\"supported_languages\"]\n",
    "        return languages\n",
    "    elif API_caller == \"account_metadata_importer\":\n",
    "        # this API call only requires the directory path to the config file (which stores a CSV file necessary)\n",
    "        return config_directory_path,\n",
    "    else:\n",
    "        print(\"Error on reading config file: no API caller specified\")\n",
    "        \n",
    "def raw_tweet_database_connector():\n",
    "    \"\"\"\n",
    "    Creates and returns a connection object to a PostgreSQL database.\n",
    "    \n",
    "    :return: (psycopg2.connect) a PostgreSQL connection object\n",
    "    \"\"\"\n",
    "    config = config_file_reader(\"data_access_object\")\n",
    "    database_type, database_host, database_name, database_user, database_password, \\\n",
    "        database_instance_id, database_port = config\n",
    "    \n",
    "    try:\n",
    "        connection = psycopg2.connect(host=database_host, dbname=database_name, user=database_user, \n",
    "                                      password=database_password, port=database_port)\n",
    "        return connection\n",
    "    except psycopg2.OperationalError:\n",
    "        print('Database connection error')\n",
    "        \n",
    "def raw_tweet_database_reader() -> list:\n",
    "    \"\"\"\n",
    "    Reads the last 100 entries in the Raw Tweet Database. \n",
    "    \n",
    "    :return: (pandas dataframe) a dataframe containing the last 100 entries in the Raw Tweet Database \n",
    "    \"\"\"\n",
    "    # calls the database connector\n",
    "    connection = raw_tweet_database_connector()\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    sql = \"SELECT tweet_time_created, tweet_uid, tweet_text, tweet_source, reply_tweet_uid, reply_tweet_count, \" \\\n",
    "      \"quote_tweet, quote_tweet_uid, quote_tweet_text, quote_tweet_count, retweet_tweet_status,  \" \\\n",
    "      \"retweet_tweet_count, tweet_language, user_uid, user_name, user_screen_name, user_description, \" \\\n",
    "      \"user_verification, user_follower_count, user_friends_count, user_statuses_count, user_time_created, \" \\\n",
    "      \"tweet_coordinates, tweet_place, tweet_place_country_code, tweet_place_bounding_box, \" \\\n",
    "      \"tweet_hashtags, tweet_urls, tweet_symbols, tweet_user_mentions, user_location FROM twitter_posts \" \\\n",
    "      \"ORDER BY tweet_time_created DESC LIMIT %s;\"\n",
    "    \n",
    "    cursor.execute(sql, (number_of_rows,))\n",
    "    return cursor.fetchall()\n",
    "\n",
    "fetched_results = raw_tweet_database_reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-05 02:31:05\n",
      "\n",
      "1246626381577977857\n",
      "\n",
      "Undocumented workers among those hit first — and worst — by the coronavirus shut down https://t.co/xSHunyCcDx\n",
      "\n",
      "<a href=\"http://www.socialflow.com\" rel=\"nofollow\">SocialFlow</a>\n",
      "\n",
      "None\n",
      "\n",
      "0\n",
      "\n",
      "False\n",
      "\n",
      "None\n",
      "\n",
      "None\n",
      "\n",
      "0\n",
      "\n",
      "None\n",
      "\n",
      "0\n",
      "\n",
      "en\n",
      "\n",
      "2467791\n",
      "\n",
      "The Washington Post\n",
      "\n",
      "washingtonpost\n",
      "\n",
      "Breaking news, analysis, and opinion. Founded in 1877. Our staff on Twitter: https://twitter.com/washingtonpost/lists/washington-post-people\n",
      "\n",
      "True\n",
      "\n",
      "15357213\n",
      "\n",
      "1660\n",
      "\n",
      "354175\n",
      "\n",
      "2007-03-27 11:19:39\n",
      "\n",
      "None\n",
      "\n",
      "None\n",
      "\n",
      "None\n",
      "\n",
      "None\n",
      "\n",
      "None\n",
      "\n",
      "['https://wapo.st/39JcYKz']\n",
      "\n",
      "None\n",
      "\n",
      "None\n",
      "\n",
      "Washington, DC\n"
     ]
    }
   ],
   "source": [
    "# shows one row of data from the database\n",
    "print(*fetched_results[2], sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to initialize spaCy and load all of its dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# imports the medium-sized English-language spaCy trained module, with vectors\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step of our text processing, we need to extract all of the named \n",
    "entities from the tweet text. We will do this by first running the entire collected stream \n",
    "through a spaCy pipeline. \n",
    "\n",
    "We start by creating a list of only the tweet text data, then running that list through spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every March, a rattlesnake festival brings about $8.3 million into the local economy of Sweetwater, Texas. The tradition dates back to 1958. \n",
      "\n",
      "Photographer Lizzie Chen wanted to explore why it has remained for 62 years. https://t.co/iGiOZ6dlg2 https://t.co/UYFfmKwgRu\n",
      "\t {'user_location': 'None'}\n",
      "\t Every March DATE\n",
      "\t about $8.3 million MONEY\n",
      "\t Sweetwater GPE\n",
      "\t Texas GPE\n",
      "\t 1958 DATE\n",
      "\t Lizzie Chen PERSON\n",
      "\t 62 years DATE\n",
      "\t https://t.co/iGiOZ6dlg2 https://t.co/UYFfmKwgRu PERSON\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we create a tuple of the data we want spaCy to ingest from the tweet_text and user_location fields\n",
    "fetched_tweet_text = []\n",
    "fetched_user_location = []\n",
    "fetched_data_tuples = []\n",
    "for _ in fetched_results:\n",
    "    # list index of text tweet data within each tweet object\n",
    "    fetched_tweet_text.append(_[2])   \n",
    "    fetched_user_location.append(_[-1])\n",
    "\n",
    "# create a list of tuples of (tweet_text, user_location)\n",
    "for text, location in zip(fetched_tweet_text, fetched_user_location):\n",
    "    fetched_data_tuples.append((text, {'user_location': location}))\n",
    "\n",
    "# creates a spaCy pipe, which processes input text data as a stream, returning a Doc object for each of those Docs\n",
    "docs = list(nlp.pipe(fetched_data_tuples, as_tuples=True))\n",
    "\n",
    "# prints out Doc data - only way to show context is to print during pipe creation. \n",
    "for doc, context in nlp.pipe(fetched_data_tuples, as_tuples=True):\n",
    "    print(doc.text)\n",
    "    print('\\t', context)\n",
    "    for ent in doc.ents:\n",
    "        print('\\t', ent.text, ent.label_)\n",
    "    print('---\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the processing on the Docs objects has already been done:: all that's left now is \n",
    "to use the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We need to be able to process location data from tweets that don't contain it. Many will \n",
    "have the data as part of the tweet text, but many won't. There are a few reasons for \n",
    "this:\n",
    "\n",
    "* context context\n",
    "If a tweet is about a person, or about a well known event, location data is not \n",
    "necessary, nor is it necessarily helpful. \n",
    "\n",
    "* user context\n",
    "If a tweet is from a small source, a local newspaper, or even a national source, often \n",
    "the context is that the _source_ is local. \n",
    "\n",
    "For the first reason, we must come up with ways to make sure that the context of the \n",
    "tweet overrides the location data, even if it is provided. We need to come up with \n",
    "ways of doing this, because of the steps we're going to take for the second reason. \n",
    "\n",
    "For the second reason, we can inject the tweet user's location data if no other \n",
    "location data exists in the tweet text. \n",
    "\n",
    "In this code block, we try to complete the entire merging process, making the context Doc part of the tweet_text Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def adding_user_location():\n",
    "    \"\"\"\n",
    "    Uses the provided 'context' data added to a spaCy Doc, runs it through its \n",
    "    own NLP pipeline to extract entity data, then returns a Doc object that \n",
    "    contains NLP metadata to be added to the original Doc object it was derived from.\n",
    "    \n",
    "    param: (dict)\n",
    "    \n",
    "    returns: (spaCy Doc)\n",
    "    \"\"\"\n",
    "    # list of Doc objects with context-added location data\n",
    "    docs_with_gpe = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        ents = [(ent.text, ent.label_) for ent in doc[0].ents]\n",
    "        # code block to determine of spaCy detected entities contain GPE\n",
    "        contains_GPE = False\n",
    "        if ents:\n",
    "            for e in ents:\n",
    "                if e[1] == \"GPE\":\n",
    "                    contains_GPE = True\n",
    "        if contains_GPE is False:\n",
    "            doc_with_gpe = doc_reconstructor(doc)\n",
    "            docs_with_gpe.append(doc_with_gpe)\n",
    "        else:\n",
    "            docs_with_gpe.append(doc[0])\n",
    "    return docs_with_gpe\n",
    "        \n",
    "        \n",
    "def doc_reconstructor(original_doc):\n",
    "    \"\"\"\n",
    "    Takes the data from the old Doc (text and context) and combines it to make and return a new Doc.\n",
    "    \"\"\"\n",
    "    combined_text_and_context = str(original_doc[0]) + '. ' + str(original_doc[1]['user_location']) + '.'\n",
    "    doc = nlp(combined_text_and_context)\n",
    "    return doc\n",
    "\n",
    "docs_with_gpe = adding_user_location()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every March, a rattlesnake festival brings about $8.3 million into the local economy of Sweetwater, Texas. The tradition dates back to 1958. \n",
      "\n",
      "Photographer Lizzie Chen wanted to explore why it has remained for 62 years. https://t.co/iGiOZ6dlg2 https://t.co/UYFfmKwgRu\n",
      "<class 'spacy.tokens.doc.Doc'>\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in docs_with_gpe:\n",
    "    print(doc)\n",
    "    print(type(doc))\n",
    "    print('\\n---\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing the Text\n",
    "\n",
    "Here we attempt a bag of words approach to each tweet, seeing what results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def token_processor():\n",
    "    # list that holds processed tokens in string form\n",
    "    processed_docs = []\n",
    "\n",
    "    for doc in docs_with_gpe:    \n",
    "        # creates list per doc\n",
    "        doc_list = []\n",
    "        # flag to determine if next token is a hashtag\n",
    "        is_hashtag = False\n",
    "        for token in doc:\n",
    "            # removes hashtags by checking if the preceding token was a hashtag, assuming that \n",
    "            # the next token would be the hashtag text; breaks from loop without adding to \n",
    "            # processed token list if token is hashtag\n",
    "            if token.text == '#':\n",
    "                is_hashtag = True\n",
    "            # checks if previous token was a hashtag character\n",
    "            if is_hashtag is False:\n",
    "                # checks if the token is an alpha character (removes numerals and punctuation)\n",
    "                if token.is_alpha is True:\n",
    "                    # checks if token is part of a stop list\n",
    "                    if token.is_stop is False:\n",
    "                        # checks if token is URL-like\n",
    "                            if token.like_url is False:\n",
    "                                # lowercases each token (uses the spaCy token's lowercase attribute)\n",
    "                                token_text = token.lemma_\n",
    "                                token_text_lemma = token_text.lower()\n",
    "                                doc_list.append(token_text_lemma)\n",
    "            # if is_hashtag has been set to True, skips processing logic and resets flag\n",
    "            else:\n",
    "                is_hashtag = False\n",
    "                \n",
    "        processed_docs.append(doc_list)\n",
    "\n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorizing the Data\n",
    "\n",
    "Here we will use a bag-of-words approach using gensim\n",
    "\n",
    "### Creating and querying a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "def gensim_processor(processed_docs):\n",
    "    # creates a Dictionary from the tokens in processed_docs\n",
    "    dictionary = Dictionary(processed_docs)\n",
    "    # list comprehension iterates over processed_docs to create a gensim mmCorpus\n",
    "    # from dictionary\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "    return corpus, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Example 1: Using gensim\n",
    "\n",
    "#### Creating a bag-of-words \n",
    "\n",
    "Here we use the gensim corpus and dictionary to see the most common terms per document and across all documents. We can also use the dictionary to look up the terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "def bag_of_words(corpus, dictionary):\n",
    "    # we use the first tweet in our tweet corpus as our reference point\n",
    "    doc = corpus[0]\n",
    "\n",
    "    # sorting the doc for frequency\n",
    "    bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # print the top 10 words in the document along with the count\n",
    "    print('\\n\\ntop 10 words in tweet')\n",
    "    for word_id, word_count in bow_doc[:10]:\n",
    "        print(dictionary.get(word_id), word_count)\n",
    "\n",
    "    # creates a defaultdict - defaultdict assigns default values to non-existent keys\n",
    "    # and by supplying the argument 'int', we ensure that nonexistent keys are automatially \n",
    "    # assigned a default value of 0, making it ideal for storing the counts of words\n",
    "    total_word_count = defaultdict(int)\n",
    "    # itertools.chain.from_iterable() allows us to iterate through a set of sequences as \n",
    "    # if they were one continuous sequence - this lets us iterate through our 'corpus' \n",
    "    # object, which is a list of lists\n",
    "    for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "        total_word_count[word_id] += word_count\n",
    "\n",
    "    # creates a sorted list from the defaultdict \n",
    "    sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # prints the top 10 words across all documents, along with their count\n",
    "    print('\\n\\ntop 10 words across entire tweet corpus')\n",
    "    for word_id, word_count in sorted_word_count[:10]:\n",
    "        print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "def tfidf(corpus):\n",
    "    # creates a new TfidfModel using the corpus\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    \n",
    "    # the weights for each token in doc [0]\n",
    "    tfidf_weights = tfidf[corpus][0]\n",
    "    \n",
    "    # sorts the weights from highest to lowest\n",
    "    sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "    \n",
    "    # prints the top 10 weighted words\n",
    "    print('\\n\\ntop 10 TF-IDF weights in tweet')\n",
    "    for term_id, weight in sorted_tfidf_weights[:10]:\n",
    "        print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1 Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "top 10 words in tweet\n",
      "bring 1\n",
      "chen 1\n",
      "date 1\n",
      "economy 1\n",
      "explore 1\n",
      "festival 1\n",
      "lizzie 1\n",
      "local 1\n",
      "march 1\n",
      "million 1\n",
      "\n",
      "\n",
      "top 10 words across entire tweet corpus\n",
      "new 39\n",
      "coronavirus 33\n",
      "york 22\n",
      "case 11\n",
      "city 11\n",
      "death 10\n",
      "health 9\n",
      "say 9\n",
      "pandemic 9\n",
      "world 8\n",
      "\n",
      "\n",
      "top 10 TF-IDF weights in tweet\n",
      "chen 0.25622879300369217\n",
      "date 0.25622879300369217\n",
      "explore 0.25622879300369217\n",
      "festival 0.25622879300369217\n",
      "lizzie 0.25622879300369217\n",
      "march 0.25622879300369217\n",
      "photographer 0.25622879300369217\n",
      "rattlesnake 0.25622879300369217\n",
      "remain 0.25622879300369217\n",
      "sweetwater 0.25622879300369217\n"
     ]
    }
   ],
   "source": [
    "processed_docs = token_processor()\n",
    "\n",
    "corpus, dictionary = gensim_processor(processed_docs)\n",
    "\n",
    "bag_of_words(corpus, dictionary)\n",
    "\n",
    "tfidf(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing Example 2: Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "\n",
    "Since we are trying to determine an unlimited, unpredictable number of topics in the tweets, we must use an unsupervised learning algorithm to get the necessary results.\n",
    "\n",
    "More to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}