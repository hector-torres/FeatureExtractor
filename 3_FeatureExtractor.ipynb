{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this cell sets up the CSV reader to read data from disk\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def import_csv(headers: bool) -> list:\n",
    "    # list of lists that will contain all tweet data\n",
    "    results = []\n",
    "\n",
    "    # makes list of all files in data directory\n",
    "    csv_directory_path = os.path.join('data', 'inputs')\n",
    "    csv_file_path = os.listdir(csv_directory_path)\n",
    "\n",
    "    for file in csv_file_path:\n",
    "        with open(os.path.join(csv_directory_path, file), mode='r') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            if headers is True:\n",
    "                next(csv_reader)\n",
    "            for line in csv_reader:\n",
    "                results.append(list(line))\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now need to initialize spaCy and load all of its dependencies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# imports the medium-sized English-language spaCy trained module, with vectors\n",
    "nlp = spacy.load('en_core_web_lg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Text Pre-processing\n",
    "\n",
    "As the first step of our text pre-processing, we need to extract all of the named\n",
    "entities from the tweet text. We will do this by first running the entire collected stream\n",
    "through a spaCy pipeline.\n",
    "\n",
    "We start by creating a list of only the tweet text data, then running that list through spaCy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# we create a tuple of the data we want spaCy to ingest from the tweet_text and user_location fields\n",
    "fetched_tweet_text = []\n",
    "fetched_user_location = []\n",
    "fetched_data_tuples = []\n",
    "for _ in fetched_results:\n",
    "    # list index of text tweet data within each tweet object\n",
    "    fetched_tweet_text.append(_[2])\n",
    "    fetched_user_location.append(_[-1])\n",
    "\n",
    "# create a list of tuples of (tweet_text, user_location)\n",
    "for text, location in zip(fetched_tweet_text, fetched_user_location):\n",
    "    fetched_data_tuples.append((text, {'user_location': location}))\n",
    "\n",
    "# creates a spaCy pipe, which processes input text data as a stream, returning a Doc object for each of those Docs\n",
    "docs = list(nlp.pipe(fetched_data_tuples, as_tuples=True))\n",
    "\n",
    "# prints out Doc data - only way to show context is to print during pipe creation.\n",
    "# for doc, context in nlp.pipe(fetched_data_tuples, as_tuples=True):\n",
    "#     print(doc.text)\n",
    "#     print('\\t', context)\n",
    "#     for ent in doc.ents:\n",
    "#         print('\\t', ent.text, ent.label_)\n",
    "#     print('---\\n')\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of the processing on the Docs objects has already been done:: all that's left now is\n",
    "to use the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to be able to process location data from tweets that don't contain it. Many will\n",
    "have the data as part of the tweet text, but many won't. There are a few reasons for\n",
    "this:\n",
    "\n",
    "* context context\n",
    "If a tweet is about a person, or about a well known event, location data is not\n",
    "necessary, nor is it necessarily helpful.\n",
    "\n",
    "* user context\n",
    "If a tweet is from a small source, a local newspaper, or even a national source, often\n",
    "the context is that the _source_ is local.\n",
    "\n",
    "For the first reason, we must come up with ways to make sure that the context of the\n",
    "tweet overrides the location data, even if it is provided. We need to come up with\n",
    "ways of doing this, because of the steps we're going to take for the second reason.\n",
    "\n",
    "For the second reason, we can inject the tweet user's location data if no other\n",
    "location data exists in the tweet text.\n",
    "\n",
    "In this code block, we try to complete the entire merging process, making the context Doc part of the tweet_text Doc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def adding_user_location() -> list:\n",
    "    \"\"\"\n",
    "    Uses the provided 'context' data added to a spaCy Doc, runs it through its\n",
    "    own NLP pipeline to extract entity data, then returns a Doc object that\n",
    "    contains NLP metadata to be added to the original Doc object it was derived from.\n",
    "\n",
    "    param: (dict)\n",
    "\n",
    "    returns: (spaCy Doc)\n",
    "    \"\"\"\n",
    "    # list of Doc objects with context-added location data\n",
    "    docs_with_gpe = []\n",
    "\n",
    "    for doc in docs:\n",
    "        ents = [(ent.text, ent.label_) for ent in doc[0].ents]\n",
    "        # code block to determine of spaCy detected entities contain GPE\n",
    "        contains_GPE = False\n",
    "        if ents:\n",
    "            for e in ents:\n",
    "                if e[1] == \"GPE\":\n",
    "                    contains_GPE = True\n",
    "        if contains_GPE is False:\n",
    "            doc_with_gpe = doc_reconstructor(doc)\n",
    "            docs_with_gpe.append(doc_with_gpe)\n",
    "        else:\n",
    "            docs_with_gpe.append(doc[0])\n",
    "    return docs_with_gpe\n",
    "\n",
    "\n",
    "def doc_reconstructor(original_doc):\n",
    "    \"\"\"\n",
    "    Takes the data from the old Doc (text and context) and combines it to make and return a new Doc.\n",
    "    \"\"\"\n",
    "    combined_text_and_context = str(original_doc[0]) + '. ' + str(original_doc[1]['user_location']) + '.'\n",
    "    doc = nlp(combined_text_and_context)\n",
    "    return doc\n",
    "\n",
    "docs_with_gpe = adding_user_location()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Pre-processing the Text\n",
    "\n",
    "Here we have the option of extracting the tokens from each tweet text instance, or\n",
    "extracting entire parts of speech from each of them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# extracting tokens\n",
    "\n",
    "def token_processor():\n",
    "    # list that holds processed tokens in string form\n",
    "    processed_docs = []\n",
    "\n",
    "    for doc in docs_with_gpe:\n",
    "        # creates list per doc\n",
    "        doc_list = []\n",
    "        # flag to determine if next token is a hashtag\n",
    "        is_hashtag = False\n",
    "        for token in doc:\n",
    "            # removes hashtags by checking if the preceding token was a hashtag, assuming that\n",
    "            # the next token would be the hashtag text; breaks from loop without adding to\n",
    "            # processed token list if token is hashtag\n",
    "            if token.text == '#':\n",
    "                is_hashtag = True\n",
    "            # checks if previous token was a hashtag character\n",
    "            if is_hashtag is False:\n",
    "                # checks if the token is an alpha character (removes numerals and punctuation)\n",
    "                if token.is_alpha is True:\n",
    "                    # checks if token is part of a stop list\n",
    "                    if token.is_stop is False:\n",
    "                        # checks if token is URL-like\n",
    "                            if token.like_url is False:\n",
    "                                # lowercases each token (uses the spaCy token's lowercase attribute)\n",
    "                                token_text = token.lemma_\n",
    "                                token_text_lemma = token_text.lower()\n",
    "                                doc_list.append(token_text_lemma)\n",
    "            # if is_hashtag has been set to True, skips processing logic and resets flag\n",
    "            else:\n",
    "                is_hashtag = False\n",
    "\n",
    "        processed_docs.append(doc_list)\n",
    "\n",
    "    return processed_docs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# extracting parts of speech\n",
    "def part_of_speech_processor(docs: list) -> list:\n",
    "    \"\"\"\n",
    "    The eventual idea is to parse out the five Ws: who, what, where, when, and why. This begins the\n",
    "    process by starting to parse the text into its constituent parts of speech, starting a basic level.\n",
    "\n",
    "    The output is a dictionary, with the following key:value pairs:\n",
    "        text:   tweet text\n",
    "        who:    any named entities that correspond with people or organizations\n",
    "        what:   verbs and noun chunks that could correspond with events\n",
    "        where:  any named entities that correspond with places (if none available, dafaults to tweet user location)\n",
    "        when:   any named entities that correspond with time (if none availble, defaults to tweet timestamp)\n",
    "        why:    TBD\n",
    "\n",
    "    :param      docs: a list of spaCy Doc object containing a sequence of tokens and their linguistic annotations\n",
    "\n",
    "    :returns    parts_of_speech: dictionary containing tweet text plus rule-based parsing of who, what, where, when, and why\n",
    "    \"\"\"\n",
    "\n",
    "    # categorizes spaCy entity types (which themselves from from the OntoNotes 5 corpus: details at https://spacy.io/api/annotation#named-entities)\n",
    "    who_types = ['NORP', 'ORG', 'PERSON']\n",
    "    what_types = ['EVENT', 'LAW']\n",
    "    where_types = ['GPE', 'LOC']\n",
    "    when_types = ['DATE', 'TIME']\n",
    "    why_types = []\n",
    "    uncat_types = ['CARDINAL' 'FAC', 'LANGUAGE', 'MONEY', 'ORDINAL', 'PERCENT', 'PRODUCT', 'QUANTITY', 'WORK_OF_ART']\n",
    "\n",
    "    parts_of_speech = []\n",
    "\n",
    "    for doc in docs:\n",
    "        who_tokens = []\n",
    "        what_tokens = []\n",
    "        where_tokens = []\n",
    "        when_tokens = []\n",
    "        why_tokens = []\n",
    "        uncat_tokens = []\n",
    "\n",
    "        noun_chunks = []\n",
    "\n",
    "        ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        if ents:\n",
    "            for e in ents:\n",
    "                if e[1] in who_types:\n",
    "                    who_tokens.append(e[0])\n",
    "                elif e[1] in what_types:\n",
    "                    what_tokens.append(e[0])\n",
    "                elif e[1] in where_types:\n",
    "                    where_tokens.append(e[0])\n",
    "                elif e[1] in when_types:\n",
    "                    when_tokens.append(e[0])\n",
    "                elif e[1] in why_types:\n",
    "                    why_tokens.append(e[0])\n",
    "                elif e[1] in uncat_types:\n",
    "                    uncat_tokens.append([e[0], e[1]])\n",
    "                else:\n",
    "                    uncat_tokens.append(['UNKNOWN CATEGORY', e[0], e[1]])\n",
    "\n",
    "        noun_chunks.append(list(doc.noun_chunks))\n",
    "\n",
    "        parts_of_speech.append([doc, who_tokens, what_tokens, where_tokens, when_tokens, why_tokens,\n",
    "                                uncat_tokens, noun_chunks])\n",
    "\n",
    "    return parts_of_speech"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "extraction_type = 'pos'\n",
    "\n",
    "if extraction_type == 'pos':\n",
    "    summarized_tweets = part_of_speech_processor(docs_with_gpe)\n",
    "elif extraction_type == 'token':\n",
    "    summarized_tweets = token_processor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Output\n",
    "\n",
    "Since this is simply a testbed for using spaCy rules to summarize text inputs, what we have\n",
    "now will be output to a CSV file for analysis and use in other applications.\n",
    "\n",
    "The output CSV will be in `data/outputs`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "now_time = str(dt.datetime.now().strftime(\"%d-%m-%Y:%H:%M:%S\"))\n",
    "\n",
    "output_directory = os.path.join('data', 'outputs')\n",
    "output_file = os.path.join(output_directory, now_time + '.csv')\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['tweet_text', 'who_tokens', 'what_tokens', 'where_tokens', 'when_tokens', 'why_tokens',\n",
    "                                'uncat_tokens', 'noun_chunks'])\n",
    "    writer.writerows(summarized_tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}