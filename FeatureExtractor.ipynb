{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The Part of Speech Tweet Summarizer\n",
    "\n",
    "This application takes in raw tweet objects from a database, then extracts summaries and\n",
    "entities from them using spaCy part-of-speech tools.\n",
    "\n",
    "## Concept\n",
    "\n",
    "### The Process\n",
    "\n",
    "In order to filter raw tweet objects for noise, then summarize them and extract their\n",
    "natural entities, we will use the following approach.\n",
    "\n",
    "1. Reading in the Dataset\n",
    "2. Pre-processing the Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Execution\n",
    "\n",
    "### 1. Reading in the Dataset\n",
    "Before we begin the feature extraction process, we need to connect to the database\n",
    "and pull down information. Two things to note as we begin this process:\n",
    "* database connection information is held in a separate configuration file inside \"/Users/\n",
    "$User/Quantum/Event Detector/Twitter Event Detector/\"\n",
    "* since the idea is to gather real-time data, the idea would be to poll the database every\n",
    "15-45 seconds. However, since we are merely testing the concept here, initial polls will be\n",
    "conducted every 120-300 seconds.\n",
    "\n",
    "Here, we connect to the database and read the last 100 entries (changeable in the first variable).\n",
    "\n",
    "#### Options\n",
    "\n",
    "We also have an option here for pulling data from a CSV file, bypassing the database collection steps, for when we have\n",
    "specific data that we want to explore using our NLP code.\n",
    "\n",
    "To do this, we set the `database` flag to False, and the code will use the CSV file contained in the `data` directory\n",
    "under the project root.\n",
    "\n",
    "Note: this CSV _must_ be formatted in the same manner as the database data, or else the code will not break. Thus, this\n",
    "is not meant to be a general parser, only to analyze specific saved data from the original database."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# this cell sets up the database connections to pull data directly\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "def config_file_reader(API_caller: str) -> tuple:\n",
    "    \"\"\"\n",
    "    A common configuration file reader.\n",
    "\n",
    "    Reads data from a common configuration file, determining which fields to call depending\n",
    "    on the API caller passed to it.\n",
    "\n",
    "    :param API_caller:(str) the name of the service calling this API\n",
    "\n",
    "    :return: (tuple) a tuple of strings of each configuration returned for the called service\n",
    "    \"\"\"\n",
    "    home_directory_path = os.path.expanduser(\"~\")\n",
    "    logger_directory_path = os.path.join(home_directory_path, \"Quantum\", \"Event Detector\",\n",
    "                                         \"Twitter Event Detector\", \"Logs\")\n",
    "    config_directory_path = os.path.join(home_directory_path, \"Quantum\", \"Event Detector\",\n",
    "                                         \"Twitter Event Detector\", \"Common\")\n",
    "    config_file_path = os.path.join(config_directory_path, \"config.ini\")\n",
    "\n",
    "    # instantiates the configuration parser\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    # if config files exists, proceed: else, create directory structure, then fail gracefully\n",
    "    if os.path.exists(config_file_path):\n",
    "        config.read(config_file_path)\n",
    "    else:\n",
    "        os.makedirs(config_directory_path)\n",
    "        print(\"No config file found in \" + config_directory_path +\n",
    "              \". Please place a configuration file into this directory and try again.\")\n",
    "\n",
    "    if API_caller == \"data_access_object\":\n",
    "        database_type = config[\"DATABASE\"][\"type\"]\n",
    "        database_host = config[\"DATABASE\"][\"host\"]\n",
    "        database_name = config[\"DATABASE\"][\"database_name\"]\n",
    "        database_user = config[\"DATABASE\"][\"user\"]\n",
    "        database_password = config[\"DATABASE\"][\"password\"]\n",
    "        database_instance_id = config[\"DATABASE\"][\"database_instance_id\"]\n",
    "        database_port = config[\"DATABASE\"][\"database_port\"]\n",
    "        return database_type, database_host, database_name, database_user, database_password, \\\n",
    "               database_instance_id, database_port\n",
    "    elif API_caller == \"logger_setup\":\n",
    "        return logger_directory_path,\n",
    "    elif API_caller == \"languages\":\n",
    "        languages = config[\"LANGUAGES\"][\"supported_languages\"]\n",
    "        return languages\n",
    "    elif API_caller == \"account_metadata_importer\":\n",
    "        # this API call only requires the directory path to the config file (which stores a CSV file necessary)\n",
    "        return config_directory_path,\n",
    "    else:\n",
    "        print(\"Error on reading config file: no API caller specified\")\n",
    "\n",
    "def raw_tweet_database_connector():\n",
    "    \"\"\"\n",
    "    Creates and returns a connection object to a PostgreSQL database.\n",
    "\n",
    "    :return: (psycopg2.connect) a PostgreSQL connection object\n",
    "    \"\"\"\n",
    "    config = config_file_reader(\"data_access_object\")\n",
    "    database_type, database_host, database_name, database_user, database_password, \\\n",
    "        database_instance_id, database_port = config\n",
    "\n",
    "    try:\n",
    "        connection = psycopg2.connect(host=database_host, dbname=database_name, user=database_user,\n",
    "                                      password=database_password, port=database_port)\n",
    "        return connection\n",
    "    except psycopg2.OperationalError:\n",
    "        print('Database connection error')\n",
    "\n",
    "def raw_tweet_database_reader() -> list:\n",
    "    \"\"\"\n",
    "    Reads the last 100 entries in the Raw Tweet Database.\n",
    "\n",
    "    :return: (pandas dataframe) a dataframe containing the last 100 entries in the Raw Tweet Database\n",
    "    \"\"\"\n",
    "    # calls the database connector\n",
    "    connection = raw_tweet_database_connector()\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    sql = \"SELECT tweet_time_created, tweet_uid, tweet_text, tweet_source, reply_tweet_uid, reply_tweet_count, \" \\\n",
    "      \"quote_tweet, quote_tweet_uid, quote_tweet_text, quote_tweet_count, retweet_tweet_status,  \" \\\n",
    "      \"retweet_tweet_count, tweet_language, user_uid, user_name, user_screen_name, user_description, \" \\\n",
    "      \"user_verification, user_follower_count, user_friends_count, user_statuses_count, user_time_created, \" \\\n",
    "      \"tweet_coordinates, tweet_place, tweet_place_country_code, tweet_place_bounding_box, \" \\\n",
    "      \"tweet_hashtags, tweet_urls, tweet_symbols, tweet_user_mentions, user_location FROM twitter_posts \" \\\n",
    "      \"ORDER BY tweet_time_created DESC LIMIT %s;\"\n",
    "\n",
    "    cursor.execute(sql, (number_of_rows,))\n",
    "    return cursor.fetchall()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# this cell sets up the CSV reader to read data from disk\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def import_csv(headers: bool) -> list:\n",
    "    # list of lists that will contain all tweet data\n",
    "    results = []\n",
    "\n",
    "    # makes list of all files in data directory\n",
    "    csv_directory_path = os.path.join('data', 'inputs')\n",
    "    csv_file_path = os.listdir(csv_directory_path)\n",
    "\n",
    "    for file in csv_file_path:\n",
    "        with open(os.path.join(csv_directory_path, file), mode='r') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            if headers is True:\n",
    "                next(csv_reader)\n",
    "            for line in csv_reader:\n",
    "                results.append(list(line))\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "number_of_rows = 100\n",
    "database = False\n",
    "headers = True\n",
    "\n",
    "if database is True:\n",
    "    fetched_results = raw_tweet_database_reader()\n",
    "else:\n",
    "    fetched_results = import_csv(headers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now need to initialize spaCy and load all of its dependencies."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# imports the medium-sized English-language spaCy trained module, with vectors\n",
    "nlp = spacy.load('en_core_web_md')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Text Pre-processing\n",
    "\n",
    "As the first step of our text pre-processing, we need to extract all of the named\n",
    "entities from the tweet text. We will do this by first running the entire collected stream\n",
    "through a spaCy pipeline.\n",
    "\n",
    "We start by creating a list of only the tweet text data, then running that list through spaCy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# we create a tuple of the data we want spaCy to ingest from the tweet_text and user_location fields\n",
    "fetched_tweet_text = []\n",
    "fetched_user_location = []\n",
    "fetched_data_tuples = []\n",
    "for _ in fetched_results:\n",
    "    # list index of text tweet data within each tweet object\n",
    "    fetched_tweet_text.append(_[2])\n",
    "    fetched_user_location.append(_[-1])\n",
    "\n",
    "# create a list of tuples of (tweet_text, user_location)\n",
    "for text, location in zip(fetched_tweet_text, fetched_user_location):\n",
    "    fetched_data_tuples.append((text, {'user_location': location}))\n",
    "\n",
    "# creates a spaCy pipe, which processes input text data as a stream, returning a Doc object for each of those Docs\n",
    "docs = list(nlp.pipe(fetched_data_tuples, as_tuples=True))\n",
    "\n",
    "# prints out Doc data - only way to show context is to print during pipe creation.\n",
    "# for doc, context in nlp.pipe(fetched_data_tuples, as_tuples=True):\n",
    "#     print(doc.text)\n",
    "#     print('\\t', context)\n",
    "#     for ent in doc.ents:\n",
    "#         print('\\t', ent.text, ent.label_)\n",
    "#     print('---\\n')\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of the processing on the Docs objects has already been done:: all that's left now is\n",
    "to use the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to be able to process location data from tweets that don't contain it. Many will\n",
    "have the data as part of the tweet text, but many won't. There are a few reasons for\n",
    "this:\n",
    "\n",
    "* context context\n",
    "If a tweet is about a person, or about a well known event, location data is not\n",
    "necessary, nor is it necessarily helpful.\n",
    "\n",
    "* user context\n",
    "If a tweet is from a small source, a local newspaper, or even a national source, often\n",
    "the context is that the _source_ is local.\n",
    "\n",
    "For the first reason, we must come up with ways to make sure that the context of the\n",
    "tweet overrides the location data, even if it is provided. We need to come up with\n",
    "ways of doing this, because of the steps we're going to take for the second reason.\n",
    "\n",
    "For the second reason, we can inject the tweet user's location data if no other\n",
    "location data exists in the tweet text.\n",
    "\n",
    "In this code block, we try to complete the entire merging process, making the context Doc part of the tweet_text Doc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def adding_user_location() -> list:\n",
    "    \"\"\"\n",
    "    Uses the provided 'context' data added to a spaCy Doc, runs it through its\n",
    "    own NLP pipeline to extract entity data, then returns a Doc object that\n",
    "    contains NLP metadata to be added to the original Doc object it was derived from.\n",
    "\n",
    "    param: (dict)\n",
    "\n",
    "    returns: (spaCy Doc)\n",
    "    \"\"\"\n",
    "    # list of Doc objects with context-added location data\n",
    "    docs_with_gpe = []\n",
    "\n",
    "    for doc in docs:\n",
    "        ents = [(ent.text, ent.label_) for ent in doc[0].ents]\n",
    "        # code block to determine of spaCy detected entities contain GPE\n",
    "        contains_GPE = False\n",
    "        if ents:\n",
    "            for e in ents:\n",
    "                if e[1] == \"GPE\":\n",
    "                    contains_GPE = True\n",
    "        if contains_GPE is False:\n",
    "            doc_with_gpe = doc_reconstructor(doc)\n",
    "            docs_with_gpe.append(doc_with_gpe)\n",
    "        else:\n",
    "            docs_with_gpe.append(doc[0])\n",
    "    return docs_with_gpe\n",
    "\n",
    "\n",
    "def doc_reconstructor(original_doc):\n",
    "    \"\"\"\n",
    "    Takes the data from the old Doc (text and context) and combines it to make and return a new Doc.\n",
    "    \"\"\"\n",
    "    combined_text_and_context = str(original_doc[0]) + '. ' + str(original_doc[1]['user_location']) + '.'\n",
    "    doc = nlp(combined_text_and_context)\n",
    "    return doc\n",
    "\n",
    "docs_with_gpe = adding_user_location()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Pre-processing the Text\n",
    "\n",
    "Here we have the option of extracting the tokens from each tweet text instance, or\n",
    "extracting entire parts of speech from each of them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# extracting tokens\n",
    "\n",
    "def token_processor():\n",
    "    # list that holds processed tokens in string form\n",
    "    processed_docs = []\n",
    "\n",
    "    for doc in docs_with_gpe:\n",
    "        # creates list per doc\n",
    "        doc_list = []\n",
    "        # flag to determine if next token is a hashtag\n",
    "        is_hashtag = False\n",
    "        for token in doc:\n",
    "            # removes hashtags by checking if the preceding token was a hashtag, assuming that\n",
    "            # the next token would be the hashtag text; breaks from loop without adding to\n",
    "            # processed token list if token is hashtag\n",
    "            if token.text == '#':\n",
    "                is_hashtag = True\n",
    "            # checks if previous token was a hashtag character\n",
    "            if is_hashtag is False:\n",
    "                # checks if the token is an alpha character (removes numerals and punctuation)\n",
    "                if token.is_alpha is True:\n",
    "                    # checks if token is part of a stop list\n",
    "                    if token.is_stop is False:\n",
    "                        # checks if token is URL-like\n",
    "                            if token.like_url is False:\n",
    "                                # lowercases each token (uses the spaCy token's lowercase attribute)\n",
    "                                token_text = token.lemma_\n",
    "                                token_text_lemma = token_text.lower()\n",
    "                                doc_list.append(token_text_lemma)\n",
    "            # if is_hashtag has been set to True, skips processing logic and resets flag\n",
    "            else:\n",
    "                is_hashtag = False\n",
    "\n",
    "        processed_docs.append(doc_list)\n",
    "\n",
    "    return processed_docs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# extracting parts of speech\n",
    "def part_of_speech_processor(docs: list) -> list:\n",
    "    \"\"\"\n",
    "    The eventual idea is to parse out the five Ws: who, what, where, when, and why. This begins the\n",
    "    process by starting to parse the text into its constituent parts of speech, starting a basic level.\n",
    "\n",
    "    The output is a dictionary, with the following key:value pairs:\n",
    "        text:   tweet text\n",
    "        who:    any named entities that correspond with people or organizations\n",
    "        what:   verbs and noun chunks that could correspond with events\n",
    "        where:  any named entities that correspond with places (if none available, dafaults to tweet user location)\n",
    "        when:   any named entities that correspond with time (if none availble, defaults to tweet timestamp)\n",
    "        why:    TBD\n",
    "\n",
    "    :param      docs: a list of spaCy Doc object containing a sequence of tokens and their linguistic annotations\n",
    "\n",
    "    :returns    parts_of_speech: dictionary containing tweet text plus rule-based parsing of who, what, where, when, and why\n",
    "    \"\"\"\n",
    "\n",
    "    # categorizes spaCy entity types (which themselves from from the OntoNotes 5 corpus: details at https://spacy.io/api/annotation#named-entities)\n",
    "    who_types = ['NORP', 'ORG', 'PERSON']\n",
    "    what_types = ['EVENT', 'LAW']\n",
    "    where_types = ['GPE', 'LOC']\n",
    "    when_types = ['DATE', 'TIME']\n",
    "    why_types = []\n",
    "    uncat_types = ['CARDINAL' 'FAC', 'LANGUAGE', 'MONEY', 'ORDINAL', 'PERCENT', 'PRODUCT', 'QUANTITY', 'WORK_OF_ART']\n",
    "\n",
    "    parts_of_speech = []\n",
    "\n",
    "    for doc in docs:\n",
    "        who_tokens = []\n",
    "        what_tokens = []\n",
    "        where_tokens = []\n",
    "        when_tokens = []\n",
    "        why_tokens = []\n",
    "        uncat_tokens = []\n",
    "\n",
    "        noun_chunks = []\n",
    "\n",
    "        ents = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        if ents:\n",
    "            for e in ents:\n",
    "                if e[1] in who_types:\n",
    "                    who_tokens.append(e[0])\n",
    "                elif e[1] in what_types:\n",
    "                    what_tokens.append(e[0])\n",
    "                elif e[1] in where_types:\n",
    "                    where_tokens.append(e[0])\n",
    "                elif e[1] in when_types:\n",
    "                    when_tokens.append(e[0])\n",
    "                elif e[1] in why_types:\n",
    "                    why_tokens.append(e[0])\n",
    "                elif e[1] in uncat_types:\n",
    "                    uncat_tokens.append([e[0], e[1]])\n",
    "                else:\n",
    "                    uncat_tokens.append(['UNKNOWN CATEGORY', e[0], e[1]])\n",
    "\n",
    "        noun_chunks.append(list(doc.noun_chunks))\n",
    "\n",
    "        parts_of_speech.append([doc, who_tokens, what_tokens, where_tokens, when_tokens, why_tokens,\n",
    "                                uncat_tokens, noun_chunks])\n",
    "\n",
    "    return parts_of_speech"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "extraction_type = 'pos'\n",
    "\n",
    "if extraction_type == 'pos':\n",
    "    summarized_tweets = part_of_speech_processor(docs_with_gpe)\n",
    "elif extraction_type == 'token':\n",
    "    summarized_tweets = token_processor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Output\n",
    "\n",
    "Since this is simply a testbed for using spaCy rules to summarize text inputs, what we have\n",
    "now will be output to a CSV file for analysis and use in other applications.\n",
    "\n",
    "The output CSV will be in `data/outputs`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "now_time = str(dt.datetime.now().strftime(\"%d-%m-%Y:%H:%M:%S\"))\n",
    "\n",
    "output_directory = os.path.join('data', 'outputs')\n",
    "output_file = os.path.join(output_directory, now_time + '.csv')\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['tweet_text', 'who_tokens', 'what_tokens', 'where_tokens', 'when_tokens', 'why_tokens',\n",
    "                                'uncat_tokens', 'noun_chunks'])\n",
    "    writer.writerows(summarized_tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}